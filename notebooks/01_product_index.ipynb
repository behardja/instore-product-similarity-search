{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b438eb60-6773-4c31-9342-461c2dd02e2f",
   "metadata": {},
   "source": [
    "# Similarity Search for Product Validation Use-Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86830307-b429-4de7-b238-b38240052a00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import requests\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint import Namespace\n",
    "from google import genai\n",
    "from google.genai.types import EmbedContentConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ccf74c-a7d1-4f9e-91dc-01c13495035b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"sandbox-401718\" # @param\n",
    "REGION = \"us-central1\" # @param\n",
    "INPUT_FILE = \"product_embeddings.jsonl\" # @param\n",
    "\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}-product-textembedding-{REGION}\"\n",
    "INPUT_URI = f\"{BUCKET_URI}/input-test\"\n",
    "OUTPUT_URI = f\"{BUCKET_URI}/output-test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646842e1-2e0b-4c7e-a8ed-56a98dadc318",
   "metadata": {},
   "source": [
    "### Create buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "147233ab-799a-4a48-a6d9-df2f13dd1138",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://sandbox-401718-product-textembedding-us-central1/...\n"
     ]
    }
   ],
   "source": [
    "# ! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f677773c-7c9e-41c0-b90b-6a945c86bda2",
   "metadata": {},
   "source": [
    "## Textembeddings on GCS\n",
    "\n",
    "Wrapped the Gemini call inside a helper function (embed_batch_sync) and then used asyncio.to_thread to run multiple batches concurrently. This gives you the speed of asynchronous processing while using the synchronous library function that works for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eac45cd-ca64-4848-8a11-cbf71015fade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5504909a-ffa2-428b-9f35-bab43bf9cf8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "\n",
    "def embed_batch_sync(batch_of_items: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    This function embeds a batch of items, preserving their original IDs.\n",
    "    It now accepts a list of dictionaries, each with an 'id' and 'content'.\n",
    "    \"\"\"\n",
    "    # 1. Extract just the content strings for the API call\n",
    "    contents_to_embed = [item[\"content\"] for item in batch_of_items]\n",
    "    \n",
    "    print(f\"Embedding a batch of {len(contents_to_embed)} items...\")\n",
    "    try:\n",
    "        # This is the synchronous API call\n",
    "        response = client.models.embed_content(\n",
    "            model=\"gemini-embedding-001\",\n",
    "            contents=contents_to_embed,  # Use the extracted content\n",
    "            config=EmbedContentConfig(\n",
    "                output_dimensionality=3072,  # Optional\n",
    "                task_type=\"RETRIEVAL_DOCUMENT\",  # Optional\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # 2. Re-associate the original ID and content with the new embedding\n",
    "        results = [\n",
    "            {\n",
    "                \"id\": item[\"id\"],\n",
    "                \"content\": item[\"content\"],\n",
    "                \"embedding\": embedding.values\n",
    "            }\n",
    "            for item, embedding in zip(batch_of_items, response.embeddings)\n",
    "        ]\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with a batch: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "async def main_async_runner(input_file: str):\n",
    "    \"\"\"Main function to read the file and run synchronous jobs concurrently.\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    tasks = []\n",
    "    with open(input_file, \"r\") as f:\n",
    "        # 3. This batch will hold dictionaries ({'id':..., 'content':...})\n",
    "        item_batch = []\n",
    "        for line in f:\n",
    "            original_data = json.loads(line)\n",
    "            item_batch.append(original_data) # Keep both id and content\n",
    "\n",
    "            if len(item_batch) >= BATCH_SIZE:\n",
    "                # Use asyncio.to_thread to run the blocking function in a separate thread\n",
    "                task = asyncio.to_thread(embed_batch_sync, item_batch)\n",
    "                tasks.append(task)\n",
    "                item_batch = []\n",
    "\n",
    "    # Process remaining items in the last batch\n",
    "    if item_batch:\n",
    "        task = asyncio.to_thread(embed_batch_sync, item_batch)\n",
    "        tasks.append(task)\n",
    "\n",
    "    print(f\"Created {len(tasks)} concurrent tasks.\")\n",
    "\n",
    "    # asyncio.gather will wait for all the threads to complete\n",
    "    all_batch_results = await asyncio.gather(*tasks)\n",
    "\n",
    "    final_results = [item for batch in all_batch_results for item in batch]\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"--- Process Finished ---\")\n",
    "    print(f\"Total time taken: {end_time - start_time:.2f} seconds\")\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f2626a-9323-4774-8d79-49f5f0cb50af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 10 concurrent tasks.\n",
      "Embedding a batch of 100 items...\n",
      "Embedding a batch of 100 items...\n",
      "Embedding a batch of 100 items...\n",
      "Embedding a batch of 100 items...\n",
      "Embedding a batch of 100 items...\n",
      "Embedding a batch of 100 items...\n",
      "Embedding a batch of 100 items...\n",
      "Embedding a batch of 100 items...\n",
      "Embedding a batch of 100 items...\n",
      "Embedding a batch of 100 items...\n",
      "--- Process Finished ---\n",
      "Total time taken: 8.38 seconds\n",
      "\n",
      "Successfully generated 1000 embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Run the main asynchronous function\n",
    "all_embeddings = await main_async_runner(INPUT_FILE)\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "if all_embeddings:\n",
    "    # 4. The 'id' column is now automatically created from your source data\n",
    "    df = pd.DataFrame(all_embeddings)\n",
    "    df['embedding_dim'] = df['embedding'].apply(len)\n",
    "    print(f\"\\nSuccessfully generated {len(df)} embeddings.\")\n",
    "else:\n",
    "    print(\"\\nNo embeddings were generated. Please check for errors in the logs above.\")\n",
    "\n",
    "# 5. Reorder columns for clarity (The 'id' is the original one from the file)\n",
    "df = df[['id', 'content', 'embedding', 'embedding_dim']]\n",
    "\n",
    "# Display the DataFrame to verify the original 'id' is used\n",
    "print(\"DataFrame with original 'id' from JSONL file:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5f4816a-7a26-4f72-ad93-aac1b5328f4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"product_embeddings_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754bf5dc-6f72-4293-ba1c-c27b0fff4cff",
   "metadata": {},
   "source": [
    "## Create Index for Vector Search\n",
    "\n",
    "This section describes the process of creating an index for Vertex Vector Search. Bruce force (exhaustive) search index is used in this example, and is used to find the exact nearest neighbors to the query vector. Brute force Index is computationally rigorous compared to ANN which is focuses on performant approximations and retrieval efficiency.\n",
    "\n",
    "For more information about the methods and their tradeoff: https://cloud.google.com/vertex-ai/docs/vector-search/create-manage-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fba2d42e-56c5-4074-8310-a66c5cb5a4a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the data in the required JSONL format for Vertex AI Vector Search\n",
    "\n",
    "data_for_index = []\n",
    "for index, row in df.iterrows():\n",
    "    data_for_index.append({\n",
    "        \"id\": str(row['id']),\n",
    "        \"embedding\": row['embedding']\n",
    "    })\n",
    "\n",
    "# Define the output file name for the index data\n",
    "INDEX_DATA_FILE_NAME = \"product_index_data.json\"\n",
    "\n",
    "# Write the data to the JSONL file\n",
    "with open(INDEX_DATA_FILE_NAME, \"w\") as f:\n",
    "    for item in data_for_index:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa745f15-82db-4099-bc7f-40f69e4ccbe4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'product_index_data.json' successfully uploaded to:\n",
      "gs://sandbox-401718-product-textembedding-us-central1/input-test/product_index_data.json\n"
     ]
    }
   ],
   "source": [
    "# Initialize the GCS client. It will infer the project from your authenticated environment.\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# Get the bucket name by simply removing the \"gs://\" prefix from your BUCKET_URI.\n",
    "bucket_name = BUCKET_URI.replace(\"gs://\", \"\")\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "# Get the destination folder path from the INPUT_URI variable.\n",
    "destination_folder = INPUT_URI.replace(BUCKET_URI, \"\").strip(\"/\")\n",
    "\n",
    "# Combine the folder path with the local filename to create the full blob name.\n",
    "# This will result in a path like \"input-test/recipes_index_data.jsonl\"\n",
    "destination_blob_name = f\"{destination_folder}/{INDEX_DATA_FILE_NAME}\"\n",
    "blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "# Upload the local file you just created.\n",
    "blob.upload_from_filename(INDEX_DATA_FILE_NAME)\n",
    "\n",
    "# Store the full GCS path of the uploaded file. This will now point to your INPUT_URI folder.\n",
    "index_data_gcs_uri = f\"gs://{bucket_name}/{destination_blob_name}\"\n",
    "\n",
    "print(f\"File '{INDEX_DATA_FILE_NAME}' successfully uploaded to:\")\n",
    "print(index_data_gcs_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b294d3f-85de-4cdb-8a13-122b6c803cce",
   "metadata": {},
   "source": [
    "## Create Index\n",
    "For similarity calculations, the documentation strongly recommends using DOT_PRODUCT_DISTANCE + UNIT_L2_NORM instead of the COSINE distance. These algorithms have been more optimized for the DOT_PRODUCT distance, and when combined with UNIT_L2_NORM, offers the same ranking and mathematical equivalence as the COSINE distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8f4c434-d229-48d1-a572-e7ac24fe2d67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=INPUT_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d1cf183-bad0-4fd4-8b79-80b13bd95e23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DIMENSIONS = len(df[\"embedding\"][0])\n",
    "DISPLAY_NAME = \"index_product_match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb6ec5f9-ee73-43bb-940e-70c92c4d518b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndex\n",
      "Create MatchingEngineIndex backing LRO: projects/757654702990/locations/us-central1/indexes/5403556491774918656/operations/7735117954135621632\n",
      "MatchingEngineIndex created. Resource name: projects/757654702990/locations/us-central1/indexes/5403556491774918656\n",
      "To use this MatchingEngineIndex in another session:\n",
      "index = aiplatform.MatchingEngineIndex('projects/757654702990/locations/us-central1/indexes/5403556491774918656')\n"
     ]
    }
   ],
   "source": [
    "ann_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    contents_delta_uri=INPUT_URI, \n",
    "    dimensions=DIMENSIONS,\n",
    "    approximate_neighbors_count=200,\n",
    "    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "    feature_norm_type=\"UNIT_L2_NORM\",\n",
    "    description=\"Similar Product match index\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cea42bb-7cc1-4712-a673-7decfdb10f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INDEX_RESOURCE_NAME = ann_index.resource_name \n",
    "\n",
    "ann_index = aiplatform.MatchingEngineIndex(\n",
    "    index_name=INDEX_RESOURCE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab8741-c465-49db-bb72-2748dc392563",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "887c410d-bfe7-49e6-b1d9-dee5c783db90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the project number\n",
    "project_number_result = !gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
    "PROJECT_NUMBER = project_number_result[0]\n",
    "# PROJECT_NUMBER = 757654702990\n",
    "\n",
    "VPC_NETWORK = \"beusebio-network\"\n",
    "VPC_NETWORK_FULL = f\"projects/{PROJECT_NUMBER}/global/networks/{VPC_NETWORK}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51716746-b280-40f8-a482-de59a39a5936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndexEndpoint\n",
      "Create MatchingEngineIndexEndpoint backing LRO: projects/757654702990/locations/us-central1/indexEndpoints/3966107766178709504/operations/4224139832135254016\n",
      "MatchingEngineIndexEndpoint created. Resource name: projects/757654702990/locations/us-central1/indexEndpoints/3966107766178709504\n",
      "To use this MatchingEngineIndexEndpoint in another session:\n",
      "index_endpoint = aiplatform.MatchingEngineIndexEndpoint('projects/757654702990/locations/us-central1/indexEndpoints/3966107766178709504')\n"
     ]
    }
   ],
   "source": [
    "# Endpoint\n",
    "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=\"index_endpoint_product\",\n",
    "    description=\"product index\",\n",
    "    network=VPC_NETWORK_FULL,\n",
    ")\n",
    "\n",
    "INDEX_ENDPOINT_NAME = my_index_endpoint.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc7ea969-76b5-44f1-9189-3e11b89b602c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying index MatchingEngineIndexEndpoint index_endpoint: projects/757654702990/locations/us-central1/indexEndpoints/3966107766178709504\n",
      "Deploy index MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/757654702990/locations/us-central1/indexEndpoints/3966107766178709504/operations/6961061768181317632\n",
      "MatchingEngineIndexEndpoint index_endpoint Deployed index. Resource name: projects/757654702990/locations/us-central1/indexEndpoints/3966107766178709504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[id: \"product_index\"\n",
       "index: \"projects/757654702990/locations/us-central1/indexes/5403556491774918656\"\n",
       "create_time {\n",
       "  seconds: 1764436645\n",
       "  nanos: 921789000\n",
       "}\n",
       "private_endpoints {\n",
       "  match_grpc_address: \"10.116.0.14\"\n",
       "}\n",
       "index_sync_time {\n",
       "  seconds: 1764437975\n",
       "  nanos: 656273000\n",
       "}\n",
       "automatic_resources {\n",
       "  min_replica_count: 2\n",
       "  max_replica_count: 2\n",
       "}\n",
       "deployment_group: \"default\"\n",
       "]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deploy\n",
    "DEPLOYED_INDEX_ID = \"product_index\"\n",
    "my_index_endpoint = my_index_endpoint.deploy_index(\n",
    "    index=ann_index, deployed_index_id=DEPLOYED_INDEX_ID\n",
    ")\n",
    "\n",
    "my_index_endpoint.deployed_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb3469f-8ef7-4722-b781-0781e1b1d200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
